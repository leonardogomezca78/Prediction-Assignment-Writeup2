<!DOCTYPE html>
<html>
<head>
<title>8.Practical Machine Learning_LG</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<h1>Practical Machine Learning</h1>
<h1>Prediction Assignment Writeup</h1>
<h2>1.	Background</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here (see the section on the Weight Lifting Exercise Dataset).</p>
<h2>2.	Loading and Processing the Raw Data</h2>
<p>The data for this project come from this source
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:</p>
<p>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</p>
<h2>3.	Data downloading</h2>
<p>We first set the default working directory and download the require training data and test data accordingly.</p>
<h4>Code_R</h4>
<pre><code>library(RCurl)
setwd(&quot;C:\\Users\\leonardo\\lgomez\\courseradatascience\\8.Practical_Machine_Learning\\Practical_Machine_Learning_Assignment&quot;)

if (!file.exists(&quot;./data&quot;)) {
    dir.create(&quot;./data&quot;)
    }
    if (!file.exists(&quot;./data/pml-training.csv&quot;)) {
    url.training &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;
    download.file(url.training, destfile = &quot;./data/pml-training.csv&quot;)
    }
    if (!file.exists(&quot;./data/pml-testing.csv&quot;)) {
    url.testing &lt;- &quot;https://d396werus45678orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;
    download.file(url.testing, destfile = &quot;./data/pml-testing.csv&quot;)
</code></pre>

<h2>4.	Reading data and data processing</h2>
<p>Continuing with the work, an exploration of the data is made</p>
<h4>Code_R</h4>
<h4>Data training and test</h4>
<pre><code>train&lt;- read.csv(&quot;./data/pml-training.csv&quot;)
test&lt;- read.csv(&quot;./data/pml-testing.csv&quot;)
dim(train)
</code></pre>

<h4>[1] 19622   160</h4>
<pre><code>dim(test)
</code></pre>

<h4>[1]  20 160</h4>
<p>Note that both dataset are having the same variables (160 variables). Next is try remove the near zero variance variables or columns that contain N/A missing values.</p>
<h4>Code_R</h4>
<pre><code>train &lt;- train[, colSums(is.na(train)) == 0] 
test &lt;- test[, colSums(is.na(test)) == 0] 
classe &lt;- train$classe
trainR &lt;- grepl(&quot;^X|timestamp|window&quot;, names(train))
train &lt;- train[, !trainR]
trainM &lt;- train[, sapply(train, is.numeric)]
trainM$classe &lt;- classe
testR &lt;- grepl(&quot;^X|timestamp|window&quot;, names(test))
test&lt;- test[, !testR]
testM &lt;- test[, sapply(test, is.numeric)]
</code></pre>

<p>There were 107 variables with more than 95% of the data missing. Those variables were removed from the data as well. If we built a classification model based on those variables, then we can expect most of the time the variable is missing and therefore we cannot apply the classification rules on them. Therefore, building a model based on variables thatâ€™s mostly missing is not practical.</p>
<h2>Correlation matrix analysis</h2>
<p>A correlation among variables is analysed before proceeding to the modeling procedures.</p>
<h4>Code_R</h4>
<pre><code>corMatrix &lt;- cor(TrainSet[, -54])
corrplot(corMatrix, order = &quot;FPC&quot;, method = &quot;color&quot;, type = &quot;lower&quot;, 
     tl.cex = 0.8, tl.col = rgb(0, 0, 0))
</code></pre>

<p>![leo1](Correlation matrix analysis.png) </p>
<h2>5.	Data Partitioning</h2>
<p>Partitioning Training data set into two data sets,70% for train data, 30% for test data as this will be used for cross validation purpose:</p>
<h4>Code_R</h4>
<pre><code>library(caret)
set.seed(12345) 
inTrain &lt;- createDataPartition(trainM$classe, p=0.70, list=F)
train_data &lt;- trainM[inTrain, ]
test_data &lt;- trainM[-inTrain, ]
</code></pre>

<h2>6.	Data Prediction and Modelling</h2>
<p>Algorithm which will be used for the predictive model here is Random Forest</p>
<h4>Code_R</h4>
<pre><code>setting &lt;- trainControl(method=&quot;cv&quot;, 5)
RandomForest &lt;- train(classe ~ ., data=train_data, method=&quot;rf&quot;, trControl=setting, ntree=250)
RandomForest

Random Forest 

13737 samples
52 predictor
 5 classes: 'A', 'B', 'C', 'D', 'E' 
No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 10989, 10990, 10990, 10989, 10990 
Resampling results across tuning parameters:

mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
2    0.9914101  0.9891330  0.001679589  0.002124719
27    0.9900270  0.9873842  0.001166909  0.001475135
52    0.9862415  0.9825951  0.001442765  0.001824660
</code></pre>

<p>Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2.</p>
<p>We estimate the performance of the model build. Getting the accuracy as well as the estimated out-of-sample error.</p>
<pre><code>predict_RandomForest &lt;- predict(RandomForest, test_data)
confusionMatrix(test_data$classe, predict_RandomForest)

Confusion Matrix and Statistics
       Reference
    Prediction    A    B    C    D    E
    A 1672    2    0    0    0
    B   12 1121    6    0    0
    C    0   19 1003    4    0
    D    0    0   27  937    0
    E    0    0    1    3 1078

Overall Statistics

           Accuracy : 0.9874          
             95% CI : (0.9842, 0.9901)
No Information Rate : 0.2862          
P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

               Kappa : 0.9841          
Mcnemar's Test P-Value : NA              

Statistics by Class:

                 Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9929   0.9816   0.9672   0.9926   1.0000
Specificity            0.9995   0.9962   0.9953   0.9945   0.9992
Pos Pred Value         0.9988   0.9842   0.9776   0.9720   0.9963
Neg Pred Value         0.9972   0.9956   0.9930   0.9986   1.0000
Prevalence             0.2862   0.1941   0.1762   0.1604   0.1832
Detection Rate         0.2841   0.1905   0.1704   0.1592   0.1832
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9962   0.9889   0.9812   0.9936   0.9996

accuracy &lt;- postResample(predict_RandomForest, test_data$classe)
error&lt;-1 - as.numeric(confusionMatrix(test_data$classe, predict_RandomForest)$overall[1])

The accuracy of the model is 98.7% and the estimated out-of-sample error is 1.3%
</code></pre>

<h2>7.	Predicting Results on the Test Data</h2>
<p>Last we will validate our model building based on the test data provided in the link</p>
<pre><code>[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
</code></pre>


</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
